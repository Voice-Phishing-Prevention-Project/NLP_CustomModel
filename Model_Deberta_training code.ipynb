{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwLzkoV-UTlS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MpaaznfrP3U"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Iy19uGyZYvG"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/이브와 모델 시도/전치리완료df.csv')\n",
        "# df.reset_index(level=0, inplace=True)  # 인덱스 값을 컬럼으로 변환\n",
        "# df.rename(columns={'index': 'idx'}, inplace=True)  # 컬럼명 변경\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmjHxBl2VIhi"
      },
      "outputs": [],
      "source": [
        "!pip install -q pytorch-lightning==1.9.4\n",
        "!pip install -q transformers==4.20.0\n",
        "!pip install -q wandb\n",
        "!pip install sentencepiece\n",
        "# # !pip install py-hanspell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIUSrE9VWanK"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModelForSequenceClassification,AutoTokenizer, AutoConfig,AdamW\n",
        "# transformers.__version__\n",
        "\n",
        "import os\n",
        "os.cpu_count()\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, SubsetRandomSampler, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import LightningDataModule\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "import wandb\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet2022')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyAKbt-SWmKT"
      },
      "outputs": [],
      "source": [
        "my_config = {\n",
        "    'project_name': 'voice_pp',\n",
        "    'model_name': 'microsoft/deberta-v3-large',\n",
        "    'num_workers': 0,  # adjust as needed\n",
        "    'num_labels': 2,\n",
        "    'max_epochs': 20,\n",
        "    'patience' : 2,\n",
        "    'max_length': 512,\n",
        "    'batch_size': 3,\n",
        "    'learning_rate':6e-6,\n",
        "    'accumulate_grad_batches': 4,\n",
        "    'warmup_steps': None,  # to be set later\n",
        "    'wandb_api_key': '본인의 키 넣기',\n",
        "    'data_dir': '/content/drive/MyDrive/이브와 모델 시도/data_dir',\n",
        "    'result_dir': '/content/drive/MyDrive/이브와 모델 시도/result_dir'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynIDJMGGhdep"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9UBnLFVi4Lu"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.reset_index(drop=True)\n",
        "train_df = train_df.reset_index()\n",
        "train_df = train_df.rename(columns = {\"index\":\"idx\"})\n",
        "test_df= test_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index()\n",
        "test_df = test_df.rename(columns = {\"index\":\"idx\"})\n",
        "# train_df[\"\"]\n",
        "\n",
        "# test_dff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-KUWKHsW3Dz"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop_duplicates()\n",
        "test_df = test_df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoE4pkUYW-Rm"
      },
      "outputs": [],
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length, train_or_test='train'):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.train_or_test = train_or_test\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.loc[idx, 'comment']\n",
        "        inputs = self.tokenizer(text,\n",
        "                                add_special_tokens=True,\n",
        "                                max_length=self.max_length,\n",
        "                                padding='max_length',\n",
        "                                truncation=True,\n",
        "                                return_tensors='pt')\n",
        "\n",
        "        if self.train_or_test == 'train':\n",
        "            label = self.df.loc[idx, 'label']\n",
        "            return {\n",
        "                'input_ids': inputs['input_ids'].squeeze(),\n",
        "                'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "                'labels': torch.tensor(label, dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'input_ids': inputs['input_ids'].squeeze(),\n",
        "                'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "                'idx': self.df.loc[idx, 'idx']\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_TwOwBtXQ__"
      },
      "outputs": [],
      "source": [
        "class NewsDataModule(LightningDataModule):\n",
        "    def __init__(self, config, tokenizer):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.tokenizer = tokenizer\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.num_workers = config['num_workers']\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # Load train and test data\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Split train data into train and validation\n",
        "        train_data, val_data = train_test_split(self.train_df, test_size=0.2, random_state=43, stratify=self.train_df['label'])\n",
        "\n",
        "        #Apply RandomOverSampler\n",
        "        ros = RandomOverSampler(random_state=43)\n",
        "        X_resampled, y_resampled = ros.fit_resample(train_data[['idx', 'comment']], train_data['label'])\n",
        "\n",
        "        # Concatenate X_resampled and y_resampled\n",
        "        resampled_data = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "        # Reset the index\n",
        "        resampled_data.reset_index(drop=True, inplace=True)\n",
        "        val_data.reset_index(drop=True, inplace=True)\n",
        "        self.test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        # Calculate the number of samples to load per epoch\n",
        "        self.num_samples_per_epoch = len(resampled_data) // 5\n",
        "        self.train_indices = list(range(len(resampled_data)))\n",
        "\n",
        "        # Create datasets\n",
        "        self.train_dataset = NewsDataset(resampled_data, self.tokenizer, self.config['max_length'], 'train')\n",
        "        self.val_dataset = NewsDataset(val_data, self.tokenizer, self.config['max_length'], 'train')\n",
        "        self.test_dataset = NewsDataset(self.test_df, self.tokenizer, self.config['max_length'], 'test')\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        sampler = SubsetRandomSampler(self.train_indices[:self.num_samples_per_epoch])\n",
        "        return DataLoader(self.train_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          sampler=sampler,\n",
        "                          num_workers = my_config['num_workers']\n",
        "                         )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers = my_config['num_workers']\n",
        "                         )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset,\n",
        "                          batch_size=self.batch_size,\n",
        "                          num_workers = my_config['num_workers']\n",
        "                         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO2xsNksCGgX"
      },
      "outputs": [],
      "source": [
        "class NewsClassifier(pl.LightningModule):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(config['model_name'], num_labels=config['num_labels'])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self(batch['input_ids'], batch['attention_mask'], batch['labels'])\n",
        "        loss = outputs.loss\n",
        "        self.log('train_loss',loss)\n",
        "        return {'loss': loss}\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self(batch['input_ids'], batch['attention_mask'], batch['labels'])\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits.argmax(dim=1)\n",
        "        labels = batch['labels'].cpu()\n",
        "\n",
        "        f1 = f1_score(labels, logits.cpu(), average='macro')\n",
        "        acc = accuracy_score(labels, logits.cpu())\n",
        "\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_f1_score', f1)\n",
        "        self.log('val_acc', acc)  # Log accuracy as 'val_acc'\n",
        "\n",
        "        return {'val_loss': loss, 'val_f1_score': f1, 'val_acc': acc}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        outputs = self(batch['input_ids'], batch['attention_mask'])\n",
        "        logits = outputs.logits.argmax(dim=1)\n",
        "        labels = batch['labels'].cpu\n",
        "        loss = outputs.loss\n",
        "        f1 = f1_score(labels, logits.cpu(), average='macro')\n",
        "        acc = accuracy_score(labels, logits.cpu())\n",
        "\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_f1_score', f1)\n",
        "        self.log('test_acc', acc)  # Log accuracy as 'val_acc'\n",
        "        return {'test_loss': loss, 'test_f1_score': f1, 'test_acc': acc}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        ids = []\n",
        "        labels = []\n",
        "        for out in outputs:\n",
        "            ids.extend(out['id'])\n",
        "            labels.extend(out['label'].cpu().tolist())  # Move the label tensor to CPU\n",
        "        # submission = pd.DataFrame({'id': ids, 'label': labels})\n",
        "        # submission.to_csv(os.path.join(self.config['result_dir'], 'submission_debertav3.csv'), index=False)\n",
        "\n",
        "        return submission\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdamW(self.parameters(), lr=self.config['learning_rate'])\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ-3QhM8Ym6X"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(my_config['model_name'])\n",
        "\n",
        "# Create dataset\n",
        "dm = NewsDataModule(my_config, tokenizer)\n",
        "dm.prepare_data()\n",
        "dm.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6NUjfaHdp47"
      },
      "outputs": [],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdbwz891YsgS"
      },
      "outputs": [],
      "source": [
        "wandb.login(key=my_config['wandb_api_key'])\n",
        "wandb_logger = WandbLogger(name = 'colab_full_debertav3',\n",
        "                           project=my_config['project_name']\n",
        "                          )\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_f1_score', mode='max', patience=my_config['patience'])\n",
        "checkpoint_callback_f1 = ModelCheckpoint(monitor='val_f1_score', mode='max', save_top_k=1, dirpath=my_config['result_dir'], filename='best_model')\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVnk3YM5dG9s"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(\n",
        "    logger=wandb_logger,\n",
        "    gpus=1,\n",
        "    max_epochs=my_config['max_epochs'],\n",
        "    callbacks=[early_stopping, checkpoint_callback_f1, lr_monitor],\n",
        "    precision=16,\n",
        "    accumulate_grad_batches=my_config['accumulate_grad_batches'],\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "news_classifier = NewsClassifier(my_config)\n",
        "# trainer.fit(news_classifier, dm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT8jnDV-qvLv"
      },
      "outputs": [],
      "source": [
        "# Test the model and save the submission results\n",
        "submission_results = trainer.test(news_classifier, datamodule=dm, ckpt_path='/content/drive/MyDrive/이브와 모델 시도/result_dir/best_model.ckpt')\n",
        "\n",
        "print(submission_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjur26EQNtE0"
      },
      "outputs": [],
      "source": [
        "trainer.test(news_classifier,\n",
        "             datamodule=dm,\n",
        "             ckpt_path='/content/drive/MyDrive/이브와 모델 시도/result_dir/best_model.ckpt'\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8sQCIV03u1i"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
